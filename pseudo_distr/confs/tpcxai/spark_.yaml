# Local filesystem on Windows, Linux, or MacOS
local_fs: &LOCAL_FS
    name: "local_fs"
    create: "tools/python/create.sh $destination"
    load: "tools/python/load.sh $destination $source"
    copy: "cp -f $source $destination"
    delete: "rm -rf $destination"
    delete_parallel: "pssh -t 0 -P -h nodes rm -rf $destination"

    download: "cp $source $destination"
# HDFS = Hadoop Distributed Filesystem
hdfs: &HDFS_local
    name: "hdfs"
    create: "tools/spark/create_hdfs.sh $destination"
    load: "tools/spark/load_hdfs.sh $destination $source"
    copy: "hdfs dfs -cp -f $source $destination"
    delete: "hdfs dfs -rm -r -f -skipTrash $destination"
    download: 'hdfs dfs -cat $source/* | awk ''BEGIN{f=""}{if($0!=f){print $0}if(NR==1){f=$0}}'' > $destination/predictions.csv'

hdfs_parallel: &HDFS
    name: "hdfs"
    create: "tools/spark/create_hdfs.sh $destination"
    load: "tools/parallel-data-load.sh nodes 1 $destination $source"
    copy: "hdfs dfs -cp -f $source $destination"
    delete: "hdfs dfs -rm -r -f -skipTrash $destination"
    download: 'hdfs dfs -cat $source/* | awk ''BEGIN{f=""}{if($0!=f){print $0}if(NR==1){f=$0}}'' > $destination/predictions.csv'

workload:
# global definitions
    engine_executable: &ENGINE "spark-submit --conf spark.executorEnv.NUMBA_CACHE_DIR=/tmp --jars '$tpcxai_home/lib/*.jar' --executor-cores 5 --num-executors 1 --driver-memory 10g --executor-memory 40g --conf spark.yarn.executor.memoryOverhead=4g --conf spark.kryoserializer.buffer.max=1g --conf spark.rpc.message.maxSize=1024 --conf spark.executor.extraJavaOptions='-Xss128m' --driver-java-options '- Xss128m' --master yarn --deploy-mode client "
    engine_executable_dl2: &ENGINE_DL2 "spark-submit --conf spark.executorEnv.NUMBA_CACHE_DIR=/tmp -- jars '$tpcxai_home/lib/*.jar' --num-executors 1 --executor-cores 1 --driver-memory 10g --executor-memory 40g --conf spark.yarn.executor.memoryOverhead=4g --conf spark.rpc.message.maxSize=1024 --conf spark.kryoserializer.buffer.max=1g --conf spark.executor.extraJavaOptions='-Xss128m' --driver-java-options '- Xss128m' --master yarn --deploy-mode client "
    engine_executable_dl5: &ENGINE_DL5 "spark-submit --conf spark.executorEnv.NUMBA_CACHE_DIR=/tmp -- jars '$tpcxai_home/lib/*.jar' --num-executors 1 --executor-cores 1 --driver-memory 10g --executor-memory 40g --conf spark.yarn.executor.memoryOverhead=4g --conf spark.rpc.message.maxSize=1024 --conf spark.kryoserializer.buffer.max=1g --conf spark.executor.extraJavaOptions='-Xss128m' --driver-java-options '- Xss128m' --master yarn --deploy-mode client "
    engine_executable_9: &SERVING_9 "spark-submit --conf spark.executorEnv.NUMBA_CACHE_DIR=/tmp --jars '$tpcxai_home/lib/*.jar' --executor-cores 5 --conf spark.task.cpus=1 --num-executors 1 --driver-memory 10g -- executor-memory 40g --conf spark.yarn.executor.memoryOverhead=4g --conf spark.kryoserializer.buffer.max=1g -- conf spark.rpc.message.maxSize=1024 --conf spark.executor.extraJavaOptions='-Xss128m' --driver-java-options '- Xss128m' --master yarn --deploy-mode client "
    datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
    training_template: &TRAINING_TEMPLATE "$engine --class $name lib/workload-assembly-0.1.jar --stage training --workdir $output $input/$file"
    serving_template: &SERVING_TEMPLATE "$engine --class $name lib/workload-assembly-0.1.jar --stage serving - -workdir $model --output $model/$phase $input/$file"
    serving_throughput_template: &SERVING_THROUGHPUT_TEMPLATE "$engine --class $name lib/workload- assembly-0.1.jar --stage serving --workdir $model --output $model/$stream $input/$file"
    training_data_url: &TRAINING_DATA_URL "output/data/training"
    serving_data_url: &SERVING_DATA_URL "output/data/serving"
    scoring_data_url: &SCORING_DATA_URL "output/raw_data/scoring"
    datagen_datastore: *LOCAL_FS
    # general/ benchmark-wide configuration parameters
    pdgf_node_parallel: True
    pdgf_home: "lib/pdgf"
    raw_data_url: "output/raw_data"
    temp_dir: '/tmp/tpcxai'
    usecases:
    1:
        # general
        name: "org.tpc.tpcxai.UseCase01"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage training --num_clusters 4 -- workdir $output $input/order.csv $input/lineitem.csv $input/order_returns.csv"
        serving_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$phase $input/order.csv $input/lineitem.csv $input/order_returns.csv"
        serving_throughput_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$stream $input/order.csv $input/lineitem.csv $input/order_returns.csv"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc01"
        output_url: "output/output/uc01"
        scoring_output_url: "output/scoring/uc01"
    2:
        # general
        name: "UseCase02.py"
        # engines
        training_engine: *ENGINE_DL2
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine $tpcxai_home/workload/spark/pyspark/workload-pyspark/$name --stage training -- epochs 25 --batch 32 --executor_cores_horovod 1 --task_cpus_horovod 1 --workdir $output '$input/$file' $input/CONVERSATION_AUDIO.seq"
        serving_template: "$engine $tpcxai_home/workload/spark/pyspark/workload-pyspark/$name --stage serving -- batch 32 --workdir $model --output $model/$phase '$input/$file' $input/CONVERSATION_AUDIO.seq"
        serving_throughput_template: "$engine $tpcxai_home/workload/spark/pyspark/workload-pyspark/$name --stage serving --batch 32 --workdir $model --output $model/$stream '$input/$file' $input/CONVERSATION_AUDIO.seq"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc02"
        output_url: "output/output/uc02"
        scoring_output_url: "output/scoring/uc02"
        working_dir: "/tmp"
    3:
        # general
        name: "org.tpc.tpcxai.UseCase03"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage training --workdir $output $input/order.csv $input/lineitem.csv $input/product.csv"
        serving_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$phase $input/store_dept.csv"
        serving_throughput_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$stream $input/store_dept.csv"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc03"
        output_url: "output/output/uc03"
        scoring_output_url: "output/scoring/uc03"
    4:
        # general
        name: "org.tpc.tpcxai.UseCase04"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: *TRAINING_TEMPLATE
        serving_template: *SERVING_TEMPLATE
        serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc04"
        output_url: "output/output/uc04"
        scoring_output_url: "output/scoring/uc04"
    5:
        # general
        name: "UseCase05.py"
        # engines
        training_engine: *ENGINE_DL5
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        # add namenode if necessary by specifying
        # $engine [path]/$name --namenode [namenode.url:port]
        training_template: &TRAINING_TEMPLATE_PY "$engine $tpcxai_home/workload/spark/pyspark/workload- pyspark/$name --stage training --epochs 15 --batch 512 --workdir $output $input/$file"
        serving_template: "$engine $tpcxai_home/workload/spark/pyspark/workload-pyspark/$name --stage serving -- batch 512 --workdir $model --output $model/$phase $input/$file"
        serving_throughput_template: "$engine $tpcxai_home/workload/spark/pyspark/workload-pyspark/$name --stage serving --batch 512 --workdir $model --output $model/$stream $input/$file"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc05"
        output_url: "output/output/uc05"
        scoring_output_url: "output/scoring/uc05"
        working_dir: "/tmp"
    6:
        # general
        name: "org.tpc.tpcxai.UseCase06"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: *TRAINING_TEMPLATE
        serving_template: *SERVING_TEMPLATE
        serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc06"
        output_url: "output/output/uc06"
        scoring_output_url: "output/scoring/uc06"
    7:
        # general
        name: "org.tpc.tpcxai.UseCase07"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage training --num-blocks 20 --workdir $output $input/$file"
        serving_template: *SERVING_TEMPLATE
        serving_throughput_template: *SERVING_THROUGHPUT_TEMPLATE
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc07"
        output_url: "output/output/uc07"
        scoring_output_url: "output/scoring/uc07"
    8:
        # general
        name: "org.tpc.tpcxai.UseCase08"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage training --num-workers 1 --num- threads 1 --workdir $output $input/order.csv $input/lineitem.csv $input/product.csv"
        serving_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --num-workers 1 --num- threads 1 --workdir $model --output $model/$phase $input/order.csv $input/lineitem.csv $input/product.csv"
        serving_throughput_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --num-workers 1 --num-threads 1 --workdir $model --output $model/$stream $input/order.csv $input/lineitem.csv $input/product.csv"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc08"
        output_url: "output/output/uc08"
        scoring_output_url: "output/scoring/uc08"
    9:
        # general
        name: "UseCase09.py"
        # engines
        training_engine: *ENGINE_DL2
        serving_engine: *SERVING_9
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        # add namenode if necessary by specifying
        # $engine [path]/$name --namenode [namenode.url:port]
        training_template: "$engine --files $tpcxai_home/workload/spark/pyspark/workload- pyspark/resources/uc09/shape_predictor_5_face_landmarks.dat $tpcxai_home/workload/spark/pyspark/workload- pyspark/$name --stage training --epochs_embedding=15 --batch=64 --executor_cores_horovod 1 -- task_cpus_horovod 1 --workdir $output '$input/CUSTOMER_IMAGES_META.csv' '$input/CUSTOMER_IMAGES.seq'"
        serving_template: "$engine --files $tpcxai_home/workload/spark/pyspark/workload- pyspark/resources/uc09/shape_predictor_5_face_landmarks.dat $tpcxai_home/workload/spark/pyspark/workload- TPC Express AI (TPCx-AI) Standard Specification Revision 1.0.1 - Page 80 of 85 pyspark/$name --stage serving --workdir $model --output $model/$phase '$input/CUSTOMER_IMAGES_META.csv' '$input/CUSTOMER_IMAGES.seq'"
        serving_throughput_template: "$engine --files $tpcxai_home/workload/spark/pyspark/workload- pyspark/resources/uc09/shape_predictor_5_face_landmarks.dat $tpcxai_home/workload/spark/pyspark/workload- pyspark/$name --stage serving --workdir $model --output $model/$stream '$input/CUSTOMER_IMAGES_META.csv' '$input/CUSTOMER_IMAGES.seq'"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc09"
        output_url: "output/output/uc09"
        scoring_output_url: "output/scoring/uc09"
        working_dir: "/tmp"
    10:
        # general
        name: "org.tpc.tpcxai.UseCase10"
        # engines
        training_engine: *ENGINE
        serving_engine: *ENGINE
        # data stores
        training_datastore: *HDFS # for storing the training data
        model_datastore: *HDFS # for storing the trained models
        serving_datastore: *HDFS # for storing the serving data
        output_datastore: *HDFS # for storing the final output
        # templates
        datagen_template: "java -jar $pdgf -ns -sf $scale_factor -s $table"
        training_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage training --workdir $output $input/financial_account.csv $input/financial_transactions.csv"
        serving_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$phase $input/financial_account.csv $input/financial_transactions.csv"
        serving_throughput_template: "$engine --class $name lib/workload-assembly-0.1.jar --stage serving --workdir $model --output $model/$stream $input/financial_account.csv $input/financial_transactions.csv"
        # URLs
        training_data_url: *TRAINING_DATA_URL
        serving_data_url: *SERVING_DATA_URL
        scoring_data_url: *SCORING_DATA_URL
        model_url: "output/model/uc10"
        output_url: "output/output/uc10"
        scoring_output_url: "output/scoring/uc10"
